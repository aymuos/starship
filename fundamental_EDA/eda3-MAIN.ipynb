{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7056960b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (2.3.5)\n",
      "Requirement already satisfied: fastparquet in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (2025.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cramjam>=2.3 in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (from fastparquet) (2026.1.0)\n",
      "Requirement already satisfied: packaging in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (from fastparquet) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aymuos/Documents/Github/starship/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a3693fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f5043",
   "metadata": {},
   "source": [
    "# Read roads csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caead472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    osm_id  code     fclass  name  ref oneway  maxspeed  layer bridge tunnel  \\\n",
      "0  4296533  5141    service   NaN  NaN      B         0      0      F      F   \n",
      "1  4296592  5115   tertiary   朝晖路  NaN      B         0      0      F      F   \n",
      "2  4298707  5114  secondary  体育场路  NaN      F         0      0      F      F   \n",
      "\n",
      "       city                                           geometry city_chinese  \n",
      "0  Hangzhou  LINESTRING (13373499.140421594 3538916.2947984...          杭州市  \n",
      "1  Hangzhou  LINESTRING (13376291.100042382 3539415.0794264...          杭州市  \n",
      "2  Hangzhou  LINESTRING (13375063.92510783 3538697.18123532...          杭州市  \n",
      " ----------------------------------------------------------------------------- \n",
      " ----------------------------------------------------------------------------- \n",
      "   order_id  region_id      city  courier_id        lng       lat  aoi_id  \\\n",
      "0    583722          0  Hangzhou         175  120.17895  30.26401     749   \n",
      "1   2819059          0  Hangzhou         175  120.17899  30.26336     749   \n",
      "2   2879432          0  Hangzhou         175  120.17896  30.26404     749   \n",
      "\n",
      "   aoi_type     accept_time accept_gps_time  accept_gps_lng  accept_gps_lat  \\\n",
      "0         1  10-30 09:20:00  10-30 09:20:00       120.20600        30.28657   \n",
      "1         1  10-31 09:47:00  10-31 09:47:00       120.20591        30.28651   \n",
      "2         1  10-22 10:11:00  10-22 10:11:00       120.20598        30.28668   \n",
      "\n",
      "    delivery_time delivery_gps_time  delivery_gps_lng  delivery_gps_lat    ds  \n",
      "0  10-30 10:30:00    10-30 10:30:00         120.17908          30.26392  1030  \n",
      "1  10-31 10:40:00    10-31 10:40:00         120.17884          30.26360  1031  \n",
      "2  10-22 15:03:00    10-22 15:03:00         120.17939          30.26395  1022  \n",
      " ----------------------------------------------------------------------------- \n",
      " ----------------------------------------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "# detailed_trajectory = pd.read_pickle(\"./data_with_trajectory_20s/courier_detailed_trajectory_20s.pkl.xz\",compression=\"xz\")\n",
    "# road_df=pd.read_csv(\"../LaDe/road-network/roads.csv\",sep=\"\\t\")\n",
    "\n",
    "road_df=pd.read_csv(\"../processed/roads_translated_motherfile.csv\")\n",
    "print(road_df.head(3))\n",
    "\n",
    "print(\" ----------------------------------------------------------------------------- \")\n",
    "\n",
    "print(\" ----------------------------------------------------------------------------- \")\n",
    "\n",
    "delivery = pd.read_csv(\"../LaDe/delivery/delivery_hz.csv\")\n",
    "print(delivery.head(3))\n",
    "\n",
    "print(\" ----------------------------------------------------------------------------- \")\n",
    "print(\" ----------------------------------------------------------------------------- \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc6050",
   "metadata": {},
   "source": [
    "# reading all postmanid into a folder parquet from a single pcikle file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e620caa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>postman_id</th>\n",
       "      <th>gps_time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321</td>\n",
       "      <td>106f5ac22cfd1574b196d16fed62f90d</td>\n",
       "      <td>03-21 07:31:58</td>\n",
       "      <td>3.953700e+06</td>\n",
       "      <td>3.053400e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321</td>\n",
       "      <td>106f5ac22cfd1574b196d16fed62f90d</td>\n",
       "      <td>03-21 07:32:18</td>\n",
       "      <td>3.953700e+06</td>\n",
       "      <td>3.053398e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321</td>\n",
       "      <td>106f5ac22cfd1574b196d16fed62f90d</td>\n",
       "      <td>03-21 07:32:41</td>\n",
       "      <td>3.953700e+06</td>\n",
       "      <td>3.053398e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ds                        postman_id        gps_time           lat  \\\n",
       "0  321  106f5ac22cfd1574b196d16fed62f90d  03-21 07:31:58  3.953700e+06   \n",
       "1  321  106f5ac22cfd1574b196d16fed62f90d  03-21 07:32:18  3.953700e+06   \n",
       "2  321  106f5ac22cfd1574b196d16fed62f90d  03-21 07:32:41  3.953700e+06   \n",
       "\n",
       "            lng  \n",
       "0  3.053400e+06  \n",
       "1  3.053398e+06  \n",
       "2  3.053398e+06  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectory20s = pd.read_pickle(\"../LaDe/data_with_trajectory_20s/courier_detailed_trajectory_20s.pkl.xz\",compression=\"xz\")\n",
    "\n",
    "trajectory20s.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2719790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lzma, pickle, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "PARQUET_COMPRESSION = 'snappy' \n",
    "\n",
    "DF_VAR = 'trajectory20s'                      # replace if your DataFrame variable name differs\n",
    "OUT_DIR = Path('../processed/per_postman_pickles')\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfe8b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume `df` is already in memory (loaded from pickle)\n",
    "# ensure postman_id column exists and is string\n",
    "df = globals().get(DF_VAR)\n",
    "if df is None:\n",
    "    raise SystemExit(\"DataFrame variable 'df' not found in globals(). Please assign your DataFrame to variable `df`.\")\n",
    "\n",
    "if 'postman_id' not in df.columns:\n",
    "    raise SystemExit(\"Column 'postman_id' not found in DataFrame.\")\n",
    "\n",
    "df['postman_id'] = df['postman_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "161886d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 50 parquet files to: /home/aymuos/Documents/Github/starship/processed/per_postman_pickles\n"
     ]
    }
   ],
   "source": [
    "# find top 100 postmen by record count\n",
    "top50 = df['postman_id'].value_counts().nlargest(50).index.tolist()\n",
    "\n",
    "\n",
    "# write per-postman parquet files\n",
    "for pid in top50:\n",
    "    sub = df[df['postman_id'] == pid].copy()\n",
    "    out_path = OUT_DIR / f'postman_{pid}.parquet'\n",
    "    sub.to_parquet(out_path, index=False, compression=PARQUET_COMPRESSION)\n",
    "\n",
    "print(f\"Saved {len(top50)} parquet files to: {OUT_DIR.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c28db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80082, 5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courier_1 = pd.read_parquet(\"../processed/per_postman_pickles/postman_0c032cafea40c06fa6337701caf35607.parquet\")\n",
    "courier_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3703b1",
   "metadata": {},
   "source": [
    "Key steps to perform\n",
    "- Normalize timestamps and sort by time per postman.\n",
    "- Drop exact duplicates (same timestamp+coords).\n",
    "- Remove outliers (bad coords, nan lat/lng).\n",
    "- Compute step metrics: dt (seconds), haversine distance (m), speed = distance / dt.\n",
    "- Remove zero/near-zero-movement duplicates: consecutive points with distance < d_eps (e.g., 2–5 m) and dt small—keep only the first in run.\n",
    "- Collapse long stops: detect stationary segments where speed < v_thresh (e.g., 0.5 m/s) for duration >= t_stop (e.g., 10+ minutes); keep only one representative point per stop (start point or centroid + aggregated stats).\n",
    "- Remove implausible jumps: speed > v_max (e.g., 40 m/s ~ 144 km/h) — drop or interpolate.\n",
    "- Optional smoothing: rolling median on lat/lng to reduce GPS jitter before stop detection.\n",
    "- Save cleaned per-postman parquet.\n",
    "- Recommended default thresholds (adjust to your city/device)\n",
    "\n",
    "- d_eps = 5 meters\n",
    "- v_thresh = 0.5 m/s\n",
    "- t_stop = 600 seconds (10 min)\n",
    "- v_max = 40 m/s\n",
    "- min_dt = 5 seconds (ignore dt=0)\n",
    "- Concise runnable example (apply per parquet file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e46553",
   "metadata": {},
   "source": [
    "## idea -> \n",
    "\n",
    "Aggregation rules (LaDe repo -inspired)\n",
    "For stationary segments longer than t_stop seconds, collapse into one aggregated \"stop\" with centroid, duration, total_dist, mean_speed, n_points.\n",
    "For movement segments and short stops, keep endpoints and a summary segment row.\n",
    "Produce cleaned trajectory with representative points (stop centroids or segment endpoints).\n",
    "Why: long stops (courier waiting/serving) contribute little to route geometry but matter as tasks; collapsing reduces data volume while preserving task-level info for modeling. Keeping endpoints for moves preserves route shape with much fewer points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ed91b5",
   "metadata": {},
   "source": [
    "- Historical averages\n",
    "What: optional merge of a history DataFrame (previous segment records) to compute per-cluster historical average duration and visit counts, attaching these to new segments.\n",
    "Why: LaDe-style time prediction benefits from historical statistics per stop (how long stops typically last, how often visited) to predict future durations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03ed9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Haversine (meters)\n",
    "def haversine_m(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000.0\n",
    "    phi1 = np.radians(lat1); phi2 = np.radians(lat2)\n",
    "    dphi = phi2 - phi1\n",
    "    dl = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2.0)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dl/2.0)**2\n",
    "    return R * 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1afea871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters \n",
    "\n",
    "MIN_MOVEMENT_DISTANCE = 5.0         # meters to consider movement\n",
    "MAX_SPEED = 40.0,        # max plausible speed m/s\n",
    "SECS_AGGREGATED = 600   # 10 mins aggregate \n",
    "\n",
    "DBSCAN_CLUSTER = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d8b738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean dataframe \n",
    "\n",
    "def process_postman(df,\n",
    "                    postman_id,\n",
    "                    d_eps=MIN_MOVEMENT_DISTANCE , \n",
    "                    v_max= MAX_SPEED,\n",
    "                    t_stop=SECS_AGGREGATED ,\n",
    "                    min_dt=1,\n",
    "                    stop_cluster_eps=DBSCAN_CLUSTER,   # meters for DBSCAN stop clustering\n",
    "                    stop_cluster_min_samples=1,\n",
    "                    history_df=None):      # optional DataFrame of past segments for historical averages\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      df: DataFrame for a single postman (may contain multiple days). Must have columns lat,lng and timestamp or ds+gps_time.\n",
    "      postman_id: identifier (string/int)\n",
    "      history_df: optional aggregated segments from previous runs to compute historical averages (same schema as segments_df)\n",
    "\n",
    "    Returns:\n",
    "      cleaned_df: reduced trajectory (representative points)\n",
    "      segments_df: aggregated segments with LaDe-style features\n",
    "      stop_clusters: DataFrame of identified stop clusters (centroid, cluster_id, count, avg_duration)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- normalize timestamps ---\n",
    "    if 'timestamp' not in df.columns:\n",
    "        \n",
    "        \n",
    "        df['timestamp'] = pd.to_datetime(df.get('gps_time', None), errors='coerce')\n",
    "    df = df.dropna(subset=['lat','lng','timestamp']).copy()\n",
    "\n",
    "\n",
    "    df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "    df['lng'] = pd.to_numeric(df['lng'], errors='coerce')\n",
    "\n",
    "\n",
    "    df = df.dropna(subset=['lat','lng','timestamp'])\n",
    "\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    if df.empty:\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    # --- compute step metrics ---\n",
    "    df['lat_p'] = df['lat'].shift(1)  # returns a new DataFrame (or Series) where each value is shifted down by 1 row:\n",
    "    df['lng_p'] = df['lng'].shift(1)\n",
    "    df['t_p'] = df['timestamp'].shift(1)\n",
    "\n",
    "\n",
    "    df['dt'] = (df['timestamp'] - df['t_p']).dt.total_seconds().fillna(0)\n",
    "    df['dt_fixed'] = df['dt'].replace(0, min_dt)\n",
    "\n",
    "\n",
    "    # haversine between previous and cect \n",
    "    df['dist'] = haversine_m(df['lat_p'].fillna(df['lat']), df['lng_p'].fillna(df['lng']), df['lat'], df['lng'])\n",
    "\n",
    "\n",
    "    df['speed'] = df['dist'] / df['dt_fixed']\n",
    "\n",
    "    \n",
    "    # filter implausible jumps\n",
    "    df = df[(df['speed'] <= v_max) | (df['dt'] == 0)].copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # --- mark move vs stationary and segment ids ---\n",
    "    df['is_move'] = df['dist'] >= d_eps\n",
    "    df['seg_change'] = (df['is_move'] != df['is_move'].shift(1)).fillna(True)\n",
    "    df['seg_id'] = df['seg_change'].cumsum()\n",
    "\n",
    "    # --- aggregate segments into LaDe-style rows ---\n",
    "    seg_rows = []\n",
    "    cleaned_rows = []\n",
    "    stop_centroids = []\n",
    "    for seg_id, g in df.groupby('seg_id', sort=True):\n",
    "        seg_is_move = bool(g['is_move'].iloc[0])\n",
    "        start_ts = g['timestamp'].iloc[0]\n",
    "        end_ts = g['timestamp'].iloc[-1]\n",
    "        duration = (end_ts - start_ts).total_seconds()\n",
    "        n_points = len(g)\n",
    "        total_dist = g['dist'].sum()\n",
    "        mean_speed = total_dist / max(duration, 1.0) if duration > 0 else 0.0\n",
    "        weights = g['dt'].fillna(0).values\n",
    "        if weights.sum() > 0:\n",
    "            centroid_lat = (g['lat'] * weights).sum() / weights.sum()\n",
    "            centroid_lng = (g['lng'] * weights).sum() / weights.sum()\n",
    "        else:\n",
    "            centroid_lat = g['lat'].mean()\n",
    "            centroid_lng = g['lng'].mean()\n",
    "\n",
    "        if (not seg_is_move) and duration >= t_stop:\n",
    "            # aggregated stop segment\n",
    "            seg_rows.append({\n",
    "                'postman_id': postman_id,\n",
    "                'seg_id': seg_id,\n",
    "                'type': 'stop',\n",
    "                'start_ts': start_ts,\n",
    "                'end_ts': end_ts,\n",
    "                'duration_s': duration,\n",
    "                'n_points': n_points,\n",
    "                'centroid_lat': centroid_lat,\n",
    "                'centroid_lng': centroid_lng,\n",
    "                'total_dist_m': total_dist,\n",
    "                'mean_speed_m_s': mean_speed\n",
    "            })\n",
    "            cleaned_rows.append({\n",
    "                'postman_id': postman_id,\n",
    "                'timestamp': start_ts,\n",
    "                'lat': centroid_lat,\n",
    "                'lng': centroid_lng,\n",
    "                'note': 'stop_agg',\n",
    "                'seg_id': seg_id\n",
    "            })\n",
    "            stop_centroids.append((centroid_lat, centroid_lng, duration))\n",
    "        else:\n",
    "            # movement or short stop: keep endpoints\n",
    "            first = g.iloc[0]\n",
    "            last = g.iloc[-1]\n",
    "            cleaned_rows.append({\n",
    "                'postman_id': postman_id,\n",
    "                'timestamp': first['timestamp'],\n",
    "                'lat': first['lat'],\n",
    "                'lng': first['lng'],\n",
    "                'note': 'seg_start',\n",
    "                'seg_id': seg_id\n",
    "            })\n",
    "            if n_points > 1:\n",
    "                cleaned_rows.append({\n",
    "                    'postman_id': postman_id,\n",
    "                    'timestamp': last['timestamp'],\n",
    "                    'lat': last['lat'],\n",
    "                    'lng': last['lng'],\n",
    "                    'note': 'seg_end',\n",
    "                    'seg_id': seg_id\n",
    "                })\n",
    "            seg_rows.append({\n",
    "                'postman_id': postman_id,\n",
    "                'seg_id': seg_id,\n",
    "                'type': 'move' if seg_is_move else 'short_stop',\n",
    "                'start_ts': start_ts,\n",
    "                'end_ts': end_ts,\n",
    "                'duration_s': duration,\n",
    "                'n_points': n_points,\n",
    "                'centroid_lat': centroid_lat,\n",
    "                'centroid_lng': centroid_lng,\n",
    "                'total_dist_m': total_dist,\n",
    "                'mean_speed_m_s': mean_speed\n",
    "            })\n",
    "\n",
    "    cleaned_df = pd.DataFrame(cleaned_rows).sort_values('timestamp').reset_index(drop=True)\n",
    "    segments_df = pd.DataFrame(seg_rows).sort_values('start_ts').reset_index(drop=True)\n",
    "\n",
    "    # --- identify stop clusters (DBSCAN) from stop centroids and also from segments' centroids of type 'stop' ---\n",
    "    stop_rows = segments_df[segments_df['type']=='stop']\n",
    "    stop_clusters = pd.DataFrame()\n",
    "    if not stop_rows.empty:\n",
    "        coords = stop_rows[['centroid_lat','centroid_lng']].to_numpy()\n",
    "        # convert meters eps to radians approx? DBSCAN with haversine requires coords in radians and metric='haversine'\n",
    "        # simpler: convert eps meters -> degrees approx (~111111 m per degree lat). Use eps_deg = eps_m / 111111\n",
    "        eps_deg = stop_cluster_eps / 111111.0\n",
    "        clustering = DBSCAN(eps=eps_deg, min_samples=stop_cluster_min_samples, metric='haversine' if False else 'euclidean')\n",
    "        # DBSCAN on lat/lng in degrees with euclidean approximates small eps well\n",
    "        labels = clustering.fit_predict(coords)\n",
    "        stop_rows = stop_rows.assign(cluster_id=labels)\n",
    "        # compute cluster summaries\n",
    "        clusters = stop_rows.groupby('cluster_id').agg(\n",
    "            cluster_count=('seg_id','count'),\n",
    "            avg_duration_s=('duration_s','mean'),\n",
    "            centroid_lat=('centroid_lat','mean'),\n",
    "            centroid_lng=('centroid_lng','mean')\n",
    "        ).reset_index()\n",
    "        stop_clusters = clusters\n",
    "        # attach cluster id to segments_df\n",
    "        segments_df = segments_df.merge(stop_rows[['seg_id','cluster_id']], on='seg_id', how='left')\n",
    "\n",
    "    # --- historical averages (from history_df) ---\n",
    "    # compute per-cluster historical avg duration and counts if history provided\n",
    "    if history_df is not None and not history_df.empty:\n",
    "        # expect history_df has cluster_id, duration_s, centroid coords\n",
    "        hist = history_df.copy()\n",
    "        # if history has cluster coordinates, match by nearest cluster centroid (use simple join by rounding coords)\n",
    "        if 'cluster_id' in hist.columns:\n",
    "            # compute per-cluster aggregates\n",
    "            hist_agg = hist.groupby('cluster_id').agg(\n",
    "                hist_avg_duration_s=('duration_s','mean'),\n",
    "                hist_visit_count=('seg_id','count')\n",
    "            ).reset_index()\n",
    "            stop_clusters = stop_clusters.merge(hist_agg, on='cluster_id', how='left')\n",
    "            # attach historical estimates to segments_df via cluster_id\n",
    "            segments_df = segments_df.merge(hist_agg, on='cluster_id', how='left')\n",
    "        else:\n",
    "            # fallback: compute global historical averages\n",
    "            gavg = hist['duration_s'].mean()\n",
    "            segments_df['hist_avg_duration_s'] = gavg\n",
    "    else:\n",
    "        # set NaNs\n",
    "        segments_df['hist_avg_duration_s'] = np.nan\n",
    "\n",
    "    # --- additional LaDe-style features per segment ---\n",
    "    # time-based features\n",
    "    segments_df['start_hour'] = pd.to_datetime(segments_df['start_ts']).dt.hour\n",
    "    segments_df['start_dow'] = pd.to_datetime(segments_df['start_ts']).dt.dayofweek\n",
    "    segments_df['is_night'] = segments_df['start_hour'].isin(list(range(0,7)))  # example\n",
    "    # relative features\n",
    "    segments_df['dist_per_point'] = segments_df['total_dist_m'] / segments_df['n_points'].replace(0,1)\n",
    "\n",
    "    # return cleaned trajectory, segments, stop clusters\n",
    "    return cleaned_df, segments_df, stop_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "166c4ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_98150/3745643329.py:28: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['timestamp'] = pd.to_datetime(df.get('gps_time', None), errors='coerce')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../processed/per_postman_pickles/postman_0c032cafea40c06fa6337701caf35607.parquet\")  # raw loaded\n",
    "cleaned, segments, clusters = process_postman(df, postman_id='321', history_df=None)\n",
    "\n",
    "\n",
    "cleaned.head(4)\n",
    "# # save outputs\n",
    "# cleaned.to_parquet('per_postman_cleaned/postman_321.parquet', index=False)\n",
    "# segments.to_parquet('per_postman_segments/postman_321_segments.parquet', index=False)\n",
    "# clusters.to_parquet('per_postman_clusters/postman_321_clusters.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a18e02ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# traj_df[\"gps_time\"]=pd.to_datetime(traj_df[\"gps_time\"])\n",
    "\n",
    "# traj_gsort_df = (\n",
    "#     traj_df.groupby(\"postman_id\",group_keys=False)\n",
    "#     .apply(lambda g:g.sort_values(\"gps_time\"))\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# traj_basegroup_df = (\n",
    "#     traj_df.groupby([\"ds\",\"postman_id\"])\n",
    "#     .size()\n",
    "#     .reset_index(name=\"count_coordinates\")\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e4921d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['osm_id', 'code', 'fclass', 'name', 'ref', 'oneway', 'maxspeed',\n",
       "       'layer', 'bridge', 'tunnel', 'city', 'geometry', 'city_chinese'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "road_df.columns\n",
    "\n",
    "# osm_id = openstreetmap_id \n",
    "# 'code' = osm internal code schema \n",
    "# 'fclass', =  functional category of road -  motorway, primary, secondary, residential\n",
    "#  'name', = official name availble in osm\n",
    "#  'ref'   = Reference code (like highway numbers, e.g., “G60” for a national expressway). ex. S102 \n",
    "#  'oneway', = Indicates if the road is one-way (yes/no). Critical for routing algorithms.\n",
    "#  'maxspeed'= Maximum legal speed limit on the road segment (km/h).\n",
    "#  'layer' = Vertical layer of the road (used when roads overlap, e.g., flyovers vs. tunnels) --> \n",
    "#  'bridge'= Boolean flag (yes/no) indicating if the segment is a bridge.\n",
    "#  'tunnel'= Boolean flag (yes/no) indicating if the segment is a tunnel.\n",
    "#  'city', = The city to which the road belongs (Shanghai, Hangzhou, Chongqing, Jilin, Yantai).\n",
    "# 'geometry' = The polyline coordinates (latitude/longitude pairs) describing the exact shape of the road segment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206a0662",
   "metadata": {},
   "source": [
    "# Messing with the trajectory data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U5'), dtype('int64')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUFuncTypeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trajectory20s[\u001b[33m'\u001b[39m\u001b[33mds\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(\u001b[33;43m'\u001b[39;49m\u001b[33;43m2021-\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m+\u001b[49m\u001b[43mtrajectory20s\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mds\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mformat\u001b[39m=\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m, errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m trajectory20s.head(\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/starship/.venv/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/starship/.venv/lib/python3.12/site-packages/pandas/core/arraylike.py:190\u001b[39m, in \u001b[36mOpsMixin.__radd__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__radd__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__radd__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mradd\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/starship/.venv/lib/python3.12/site-packages/pandas/core/series.py:6154\u001b[39m, in \u001b[36mSeries._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6152\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[32m   6153\u001b[39m     \u001b[38;5;28mself\u001b[39m, other = \u001b[38;5;28mself\u001b[39m._align_for_op(other)\n\u001b[32m-> \u001b[39m\u001b[32m6154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/starship/.venv/lib/python3.12/site-packages/pandas/core/base.py:1391\u001b[39m, in \u001b[36mIndexOpsMixin._arith_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1388\u001b[39m     rvalues = np.arange(rvalues.start, rvalues.stop, rvalues.step)\n\u001b[32m   1390\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(\u001b[38;5;28mall\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m     result = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43marithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(result, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/starship/.venv/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:283\u001b[39m, in \u001b[36marithmetic_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    279\u001b[39m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    281\u001b[39m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[32m    282\u001b[39m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     res_values = \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/starship/.venv/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:218\u001b[39m, in \u001b[36m_na_arithmetic_op\u001b[39m\u001b[34m(left, right, op, is_cmp)\u001b[39m\n\u001b[32m    215\u001b[39m     func = partial(expressions.evaluate, op)\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    221\u001b[39m         left.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    222\u001b[39m     ):\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[32m    226\u001b[39m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Github/starship/.venv/lib/python3.12/site-packages/pandas/core/roperator.py:11\u001b[39m, in \u001b[36mradd\u001b[39m\u001b[34m(left, right)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mradd\u001b[39m(left, right):\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mright\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\n",
      "\u001b[31mUFuncTypeError\u001b[39m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U5'), dtype('int64')) -> None"
     ]
    }
   ],
   "source": [
    "trajectory20s['ds'] = pd.to_datetime('2021-'+trajectory20s['ds'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "trajectory20s.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traj_gsort_df[\"seq\"]=(\n",
    "#     traj_gsort_df.groupby(\"postman_id\")[[\"lat\",\"lng\"]]\n",
    "#     .apply(lambda g: (g != g.shift())).any(axis=1).cumsum()\n",
    "#     .reset_index(level=0, drop=True)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traj_spacegroup_df = (\n",
    "#     traj_gsort_df.groupby([\"postman_id\",\"seq\"])\n",
    "#     .agg(\n",
    "#         lat = (\"lat\",\"first\"),\n",
    "#         lng = (\"lng\",\"first\"),\n",
    "#         start_time = (\"gps_time\",\"min\"),\n",
    "#         end_time = (\"gps_time\",\"max\"),\n",
    "#         n_pings = (\"gps_time\",\"size\")\n",
    "#     )\n",
    "#     .reset_index()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c2e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# traj_spacegroup_df.to_pickle(\"trajectory_spacegroup.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
